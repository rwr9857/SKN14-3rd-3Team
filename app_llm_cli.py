import os
import json
import logging
import asyncio
from typing import List, Dict, Any, Optional, AsyncGenerator, Generator
from dotenv import load_dotenv
from pdfminer.high_level import extract_text
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_tavily import TavilySearch
from langchain_core.documents import Document
from langchain_core.callbacks.base import BaseCallbackHandler
from rag_indexer_class import IndexConfig, RAGIndexer
from utils.index import image_to_base64

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
logging.getLogger("pdfminer").setLevel(logging.ERROR)

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL_NAME = os.getenv("MODEL_NAME", "gpt-4.1-mini")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

def search_vector_db_image(img_path):
    """ë°±í„° ë””ë¹„ì—ì„œ ì´ë¯¸ì§€ì˜ ëª¨ë¸ì„ ê°€ì ¸ì˜¨ë‹¤"""

    # ì„¤ì • ìƒì„±
    config = IndexConfig(
        persistent_directory="./chroma",
        collection_name="imgs",
        embedding_model="text-embedding-3-small",
    )

    # ì¸ë±ì„œ ìƒì„± ë° ì‹¤í–‰
    indexer = RAGIndexer(config)

    # ì´ë¯¸ì§€ ë¡œë“œí•´ì„œ ëª¨ë¸ëª… ê²€ìƒ‰
    img_base64 = image_to_base64(img_path)

    # ìœ ì‚¬ë„ ê²€ìƒ‰
    model_nm = indexer.search_and_show(img_base64)
    return model_nm

class StreamingCallbackHandler(BaseCallbackHandler):
    """ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¸ë“¤ëŸ¬"""
    def __init__(self):
        self.tokens = []
        self.finished = False
    
    def on_llm_new_token(self, token: str) -> None:
        """ìƒˆ í† í°ì´ ìƒì„±ë  ë•Œ í˜¸ì¶œ"""
        self.tokens.append(token)
    
    def on_llm_end(self) -> None:
        """LLM ì‘ë‹µ ì™„ë£Œ ì‹œ í˜¸ì¶œ"""
        self.finished = True

class SmartApplianceAssistant:
    def __init__(self):
        self.embeddings_model = "text-embedding-3-small"
        self.collection_name = "manuals"
        self.vector_db_dir = "./chroma"
        self.img_collection_name = "imgs"
        
        # LLM ì´ˆê¸°í™” (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
        self.llm = ChatOpenAI(
            model=MODEL_NAME, 
            temperature=0.3,
            streaming=True
        )
        
        # ë¶„ì„ìš© LLM (ìŠ¤íŠ¸ë¦¬ë° ë¶ˆí•„ìš”)
        self.analysis_llm = ChatOpenAI(
            model=MODEL_NAME, 
            temperature=0.1,
            streaming=False
        )
        
        # RAG ì¸ë±ì„œ ì´ˆê¸°í™”
        self._initialize_indexers()
        
        # ì›¹ ê²€ìƒ‰ ë„êµ¬ ì´ˆê¸°í™”
        self.tavily_tool = TavilySearch(max_results=5)
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™”
        self._initialize_prompts()
        
        # ì²´ì¸ êµ¬ì„±
        self._setup_chains()
    
    def _initialize_indexers(self):
        """ì¸ë±ì„œ ì´ˆê¸°í™”"""
        # ë§¤ë‰´ì–¼ í…ìŠ¤íŠ¸ìš© ì¸ë±ì„œ
        self.text_config = IndexConfig(
            persistent_directory=self.vector_db_dir,
            collection_name=self.collection_name,
            embedding_model=self.embeddings_model,
        )
        self.text_indexer = RAGIndexer(self.text_config)
        
        # ì´ë¯¸ì§€ìš© ì¸ë±ì„œ
        self.img_config = IndexConfig(
            persistent_directory=self.vector_db_dir,
            collection_name=self.img_collection_name,
            embedding_model=self.embeddings_model,
        )
        self.img_indexer = RAGIndexer(self.img_config)
        
        # ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
        self.retriever = self.text_indexer.vectordb.as_retriever(
            search_type="mmr", 
            search_kwargs={"k": 8, "fetch_k": 20}
        )
    
    def _initialize_prompts(self):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™”"""
        # ì¿¼ë¦¬ ë¶„ì„ í”„ë¡¬í”„íŠ¸
        self.query_analysis_prompt = ChatPromptTemplate.from_messages([
            ('system', """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
            ì£¼ì–´ì§„ ì§ˆë¬¸ì—ì„œ ë‹¤ìŒì„ ì¶”ì¶œí•˜ì„¸ìš”:
            
            1. ì£¼ìš” í‚¤ì›Œë“œ (3-5ê°œ)
            2. ì§ˆë¬¸ì˜ í•µì‹¬ ì£¼ì œ
            3. êµ¬ì²´ì ì¸ ì¡°ê±´ì´ë‚˜ ìš”êµ¬ì‚¬í•­
            4. ë‹µë³€ì—ì„œ ë‹¤ë¤„ì•¼ í•  ì„¸ë¶€ ì‚¬í•­ë“¤
            
            JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:
            {{
                "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"],
                "main_topic": "ì£¼ì œ",
                "conditions": ["ì¡°ê±´1"],
                "details": ["ì„¸ë¶€ì‚¬í•­1"]
            }}"""),
            ("human", "ì§ˆë¬¸: {query}"),
        ])
        
        # ë©”ì¸ ë‹µë³€ í”„ë¡¬í”„íŠ¸
        self.main_answer_prompt = ChatPromptTemplate.from_messages([
            ("system", """
            ë‹¹ì‹ ì€ ìŠ¤ë§ˆíŠ¸í•œ ê°€ì „ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. Tree of Thoughts ë°©ë²•ë¡ ì„ ì‚¬ìš©í•˜ì—¬ ì²´ê³„ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
            
            ## ë‹µë³€ ì§€ì¹¨
            1. ì§ˆë¬¸ì„ ë¶„ì„í•œ í›„ ê´€ë ¨ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ì²´ê³„ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
            2. ì¡°ê±´ë“¤ì„ ë‚˜ì—´í•˜ê¸°ë³´ë‹¤ëŠ” í†µí•©í•˜ì—¬ í•˜ë‚˜ì˜ íë¦„ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”
            3. ë°˜ë³µë˜ê±°ë‚˜ ìœ ì‚¬í•œ ë‚´ìš©ì„ ì¤‘ë³µí•´ì„œ ì„¤ëª…í•˜ì§€ ë§ˆì„¸ìš”
            4. ë…¼ë¦¬ì  êµ¬ì¡°ë¥¼ ê°–ì¶˜ ëª…í™•í•œ ë¬¸ë‹¨ í˜•íƒœë¡œ ë‹µë³€í•˜ì„¸ìš”
            5. í•„ìš” ì‹œ ì˜ˆì‹œë‚˜ ìœ ì‚¬ ìƒí™©ì„ ë“¤ì–´ ì´í•´ë¥¼ ë„ìš°ì„¸ìš”
            
            ## ì¶œë ¥ í˜•ì‹
            ### í•´ê²°ì±…
            - [ì²´ê³„ì ì¸ í†µí•© ì„¤ëª…ì„ í•œ ë¬¸ë‹¨ ì´ìƒìœ¼ë¡œ ê¸°ìˆ ]
            
            ### ì¶”ê°€ ì•ˆë‚´
            - [ê´€ë ¨ëœ íŒì´ë‚˜ ì°¸ê³  ì •ë³´ê°€ ìˆìœ¼ë©´ ì œê³µ]
            """),
            ("human", """
            ì§ˆë¬¸: {query}
            ë¶„ì„ ê²°ê³¼: {analysis}
            
            ê´€ë ¨ ì •ë³´:
            {context}
            """),
        ])
    
    def _setup_chains(self):
        """ì²´ì¸ êµ¬ì„±"""
        # ì¿¼ë¦¬ ë¶„ì„ ì²´ì¸ (ìŠ¤íŠ¸ë¦¬ë° ë¶ˆí•„ìš”)
        self.query_analysis_chain = self.query_analysis_prompt | self.analysis_llm | StrOutputParser()
        
        # ë©”ì¸ ë‹µë³€ ì²´ì¸ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
        self.answer_chain = self.main_answer_prompt | self.llm
    
    def search_vector_db_image(self, img_path: str) -> str:
        """ë²¡í„° DBì—ì„œ ì´ë¯¸ì§€ì˜ ëª¨ë¸ì„ ê°€ì ¸ì˜¨ë‹¤"""
        try:
            img_base64 = image_to_base64(img_path)
            model_nm = self.img_indexer.search_and_show(img_base64)
            return model_nm if model_nm != -1 else "í™•ì¸ë¶ˆê°€"
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return "í™•ì¸ë¶ˆê°€"
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            return extract_text(pdf_path)
        except Exception as e:
            logger.error(f"PDF ì½ê¸° ì‹¤íŒ¨ {pdf_path}: {e}")
            return ""
    
    def analyze_query(self, query: str) -> Dict[str, Any]:
        """ì§ˆë¬¸ ë¶„ì„"""
        try:
            analysis_result = self.query_analysis_chain.invoke({"query": query})
            return json.loads(analysis_result)
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {
                "keywords": [query],
                "main_topic": query,
                "conditions": [],
                "details": []
            }
    
    def search_web(self, query: str) -> List[Document]:
        """ì›¹ ê²€ìƒ‰"""
        web_docs = []
        try:
            search_result = self.tavily_tool.invoke({"query": query})
            web_results = search_result.get("results", [])
            
            for item in web_results:
                content = item.get("content", "")
                url = item.get("url", "")
                
                if content:
                    doc = Document(
                        page_content=content,
                        metadata={
                            "source": url, 
                            "title": item.get("title", ""),
                            "type": "web"
                        }
                    )
                    web_docs.append(doc)
        except Exception as e:
            logger.error(f"ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        return web_docs
    
    def search_vector_db(self, keywords: List[str]) -> List[Document]:
        """ë²¡í„° DB ê²€ìƒ‰"""
        vector_docs = []
        for keyword in keywords:
            try:
                docs = self.retriever.invoke(keyword)
                for doc in docs:
                    doc.metadata["type"] = "vector"
                vector_docs.extend(docs)
            except Exception as e:
                logger.error(f"ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜ ({keyword}): {e}")
        
        return vector_docs
    
    def retrieve_contexts(self, query: str, analysis: Dict[str, Any]) -> List[Document]:
        """ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
        all_contexts = []
        
        # ì›¹ ê²€ìƒ‰
        web_docs = self.search_web(query)
        all_contexts.extend(web_docs)
        
        # ë²¡í„° DB ê²€ìƒ‰
        keywords = analysis.get("keywords", [query])
        vector_docs = self.search_vector_db(keywords)
        all_contexts.extend(vector_docs)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        unique_contexts = self._deduplicate_contexts(all_contexts)
        
        return unique_contexts
    
    def _deduplicate_contexts(self, contexts: List[Document]) -> List[Document]:
        """ì»¨í…ìŠ¤íŠ¸ ì¤‘ë³µ ì œê±°"""
        seen_content = set()
        unique_contexts = []
        
        for doc in contexts:
            content_hash = hash(doc.page_content[:100])  # ì²« 100ìë¡œ í•´ì‹œ
            if content_hash not in seen_content:
                seen_content.add(content_hash)
                unique_contexts.append(doc)
        
        return unique_contexts
    
    def format_context(self, contexts: List[Document]) -> str:
        """ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…"""
        if not contexts:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        formatted_contexts = []
        for _, doc in enumerate(contexts[:10]):  # ìµœëŒ€ 10ê°œë§Œ ì‚¬ìš©
            source = doc.metadata.get("source", "ì•Œ ìˆ˜ ì—†ìŒ")
            doc_type = doc.metadata.get("type", "unknown")
            content = doc.page_content[:500]  # ìµœëŒ€ 500ì
            
            formatted_contexts.append(f"[{doc_type.upper()}] {content}\nì¶œì²˜: {source}")
        
        return "\n\n".join(formatted_contexts)
    
    def process_query_with_image(self, query: str, image_path: str) -> str:
        """ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ì§ˆë¬¸ ì²˜ë¦¬"""
        model_code = self.search_vector_db_image(image_path)
        return f"{query} (ëª¨ë¸ì½”ë“œ: {model_code})"
    
    def chat_stream(self, query: str, image_path: Optional[str] = None, history: List[Dict[str, str]] = None) -> Generator[str, None, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… í•¨ìˆ˜"""
        if history is None:
            history = []
        
        try:
            # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ëª¨ë¸ ì½”ë“œ ì¶”ê°€
            if image_path:
                query = self.process_query_with_image(query, image_path)
            
            # 1. ì§ˆë¬¸ ë¶„ì„ (ìŠ¤íŠ¸ë¦¬ë° ì—†ì´)
            yield "ğŸ” ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...\n\n"
            analysis = self.analyze_query(query)
            logger.info(f"ì§ˆë¬¸ ë¶„ì„ ì™„ë£Œ: {analysis}")
            
            # 2. ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            yield "ğŸ“š ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤...\n\n"
            contexts = self.retrieve_contexts(query, analysis)
            formatted_context = self.format_context(contexts)
            
            # 3. ë‹µë³€ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°)
            yield "ğŸ’¡ ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...\n\n"
            
            # ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¸ë“¤ëŸ¬ ì„¤ì •
            callback_handler = StreamingCallbackHandler()
            
            # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë‹µë³€ ìƒì„±
            for chunk in self.answer_chain.stream(
                {
                    "query": query,
                    "analysis": json.dumps(analysis, ensure_ascii=False, indent=2),
                    "context": formatted_context
                },
                config={"callbacks": [callback_handler]}
            ):
                if hasattr(chunk, 'content'):
                    yield chunk.content
                
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì±—ë´‡ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            yield f"âŒ ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    async def chat_stream_async(self, query: str, image_path: Optional[str] = None, history: List[Dict[str, str]] = None) -> AsyncGenerator[str, None]:
        """ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… í•¨ìˆ˜"""
        if history is None:
            history = []
        
        try:
            # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ëª¨ë¸ ì½”ë“œ ì¶”ê°€
            if image_path:
                query = self.process_query_with_image(query, image_path)
            
            # 1. ì§ˆë¬¸ ë¶„ì„ (ë¹„ë™ê¸°)
            yield "ğŸ” ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...\n\n"
            analysis = await self.query_analysis_chain.ainvoke({"query": query})
            analysis_data = json.loads(analysis)
            logger.info(f"ì§ˆë¬¸ ë¶„ì„ ì™„ë£Œ: {analysis_data}")
            
            # 2. ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            yield "ğŸ“š ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤...\n\n"
            contexts = self.retrieve_contexts(query, analysis_data)
            formatted_context = self.format_context(contexts)
            
            # 3. ë‹µë³€ ìƒì„± (ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°)
            yield "ğŸ’¡ ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...\n\n"
            
            async for chunk in self.answer_chain.astream({
                "query": query,
                "analysis": json.dumps(analysis_data, ensure_ascii=False, indent=2),
                "context": formatted_context
            }):
                if hasattr(chunk, 'content'):
                    yield chunk.content
                
        except Exception as e:
            logger.error(f"ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì±—ë´‡ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            yield f"âŒ ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def chat(self, query: str, image_path: Optional[str] = None, history: List[Dict[str, str]] = None) -> str:
        """ì¼ë°˜ ì±„íŒ… í•¨ìˆ˜ (ìŠ¤íŠ¸ë¦¬ë° ì—†ìŒ)"""
        response_parts = []
        for chunk in self.chat_stream(query, image_path, history):
            response_parts.append(chunk)
        return "".join(response_parts)

# í¸ì˜ í•¨ìˆ˜ë“¤
def run_chatbot_stream(query: str, image_path: Optional[str] = None, history: List[Dict[str, str]] = None) -> Generator[str, None, None]:
    """ìŠ¤íŠ¸ë¦¬ë° ì±—ë´‡ ì‹¤í–‰ í•¨ìˆ˜"""
    assistant = SmartApplianceAssistant()
    yield from assistant.chat_stream(query, image_path, history)

async def run_chatbot_stream_async(query: str, image_path: Optional[str] = None, history: List[Dict[str, str]] = None) -> AsyncGenerator[str, None]:
    """ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì±—ë´‡ ì‹¤í–‰ í•¨ìˆ˜"""
    assistant = SmartApplianceAssistant()
    async for chunk in assistant.chat_stream_async(query, image_path, history):
        yield chunk

def run_chatbot(query: str, image_path: Optional[str] = None, history: List[Dict[str, str]] = None) -> str:
    """ê¸°ì¡´ í•¨ìˆ˜ì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜"""
    assistant = SmartApplianceAssistant()
    return assistant.chat(query, image_path, history)

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    import time
    
    # ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì˜ˆì‹œ
    def sync_streaming_example():
        print("=== ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì˜ˆì‹œ ===")
        assistant = SmartApplianceAssistant()
        
        query = "ì—ì–´ì»¨ í•„í„° ì²­ì†Œ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”"
        print(f"ì§ˆë¬¸: {query}\n")
        
        for chunk in assistant.chat_stream(query):
            print(chunk, end="", flush=True)
            time.sleep(0.01)  # ì‹¤ì œ ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼ë¥¼ ìœ„í•œ ì§€ì—°
        print("\n")
    
    # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì˜ˆì‹œ
    async def async_streaming_example():
        print("\n=== ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì˜ˆì‹œ ===")
        assistant = SmartApplianceAssistant()
        
        query = "ëƒ‰ì¥ê³ ì—ì„œ ì´ìƒí•œ ì†Œë¦¬ê°€ ë‚˜ëŠ”ë° ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?"
        print(f"ì§ˆë¬¸: {query}\n")
        
        async for chunk in assistant.chat_stream_async(query):
            print(chunk, end="", flush=True)
            await asyncio.sleep(0.01)  # ì‹¤ì œ ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼ë¥¼ ìœ„í•œ ì§€ì—°
        print("\n")
    
    # ì¼ë°˜ ì±„íŒ… ì˜ˆì‹œ
    def normal_chat_example():
        print("\n=== ì¼ë°˜ ì±„íŒ… ì˜ˆì‹œ ===")
        assistant = SmartApplianceAssistant()
        
        query = "ì„¸íƒê¸° ë°°ìˆ˜ê°€ ì•ˆ ë˜ëŠ” ë¬¸ì œ í•´ê²°ë²•"
        print(f"ì§ˆë¬¸: {query}\n")
        
        response = assistant.chat(query)
        print(f"ë‹µë³€: {response}\n")
    
    # ì˜ˆì‹œ ì‹¤í–‰
    sync_streaming_example()
    asyncio.run(async_streaming_example())
    normal_chat_example()